#!/usr/bin/env python3

import argparse
import json
import multiprocessing
from dataclasses import dataclass

import cloudscraper
from bs4 import BeautifulSoup
from bs4.element import ResultSet

from python.helpers.main import (
    DownloadSite,
    DownloadSitesCtx,
    EpisodeNumberNotFoundException,
    downloader_main,
)
from python.log.console import Console

BASE_URL = "https://hianime.to"

ALLOWED_SERIES_TYPES = {
    "sub",
    "dub",
    "raw",
}

UNWANTED_CDN_HOSTNAMES = {
    ".netmagcdn.com",
}


def get_zoroto_page_url(anime_name: str) -> str:
    return f"{BASE_URL}/watch/{anime_name}"


def get_anime_id(anime_name: str) -> str | None:
    Console.log_dim("Extracting id from name", return_line=True)
    name_parts = anime_name.split("-")
    anime_id = name_parts[-1]
    if anime_id.isdigit():
        return anime_id
    Console.log_dim("Fetching episode page...", return_line=True)
    response = cloudscraper.create_scraper().get(
        get_zoroto_page_url(anime_name),
        headers={"User-Agent": "Zoro.to stream video downloader"},
    )

    if not response.status_code:
        Console.log_dim("Anime not found")
        return None

    page_html = response.text

    Console.log_dim("Parsing HTML...", return_line=True)

    try:
        info = BeautifulSoup(
            page_html,
            "html.parser",
        ).find(
            id="syncData",
        )

        if info is None:
            return None

        info = info.text

        return json.loads(info)["anime_id"]
    except Exception:
        return None


@dataclass
class EpisodeInfo:
    number: float
    title: str
    id: str
    url: str


def get_page(url: str):
    return cloudscraper.create_scraper().get(
        url,
        headers={
            "Accept": "*/*",
            "X-Requested-With": "XMLHttpRequest",
            "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36",
        },
    )


def get_anime_episode_list(anime_id: str) -> dict[float, EpisodeInfo]:
    Console.log_dim("Fetching episode list...", return_line=True)
    response = get_page(
        f"{BASE_URL}/ajax/v2/episode/list/{anime_id}",
    )

    if not response.status_code:
        Console.log_dim("No episodes found")
        return {}

    page_json = response.json()

    elements = BeautifulSoup(
        page_json["html"],
        "html.parser",
    ).find_all(
        class_="ep-item",
    )

    if not elements:
        Console.log_dim("No info found")
        return {}

    return {
        float(element.attrs["data-number"]): EpisodeInfo(
            id=element.attrs["data-id"],
            url=element.attrs["href"],
            number=float(element.attrs["data-number"]),
            title=element.attrs["title"],
        )
        for element in elements
    }


@dataclass
class DownloadServerInfo:
    type: str
    id: str
    server_id: str
    name: str


def get_download_url_from_server_info(
    server: DownloadServerInfo,
) -> DownloadSite | None:
    response = get_page(
        f"{BASE_URL}/ajax/v2/episode/sources?id={server.id}",
    )

    if not response.status_code:
        return None

    page_json = response.json()

    return DownloadSite(
        name=server.name,
        type=server.type,
        url=page_json["link"],
    )


def get_anime_episode_download_server_list(
    episode: EpisodeInfo,
    series_types: list[str] | set[str],
) -> list[DownloadSite] | None:
    Console.log_dim("Fetching download servers...", return_line=True)
    response = get_page(
        f"{BASE_URL}/ajax/v2/episode/servers?episodeId={episode.id}",
    )

    if not response.status_code:
        Console.log_dim("No download servers found")
        return None

    page_json = response.json()

    elements: ResultSet = BeautifulSoup(
        page_json["html"],
        "html.parser",
    ).find_all(
        class_="server-item",
    )

    servers_infos = {
        element.attrs["data-id"]: DownloadServerInfo(
            id=element.attrs["data-id"],
            name=next(element.stripped_strings),
            server_id=element.attrs["data-server-id"],
            type=element.attrs["data-type"],
        )
        for element in elements
    }

    Console.log_dim("Fetching list of source pages...", return_line=True)

    with multiprocessing.Pool() as pool:
        results = [
            item
            for item in pool.imap_unordered(
                get_download_url_from_server_info,
                servers_infos.values(),
            )
            if item and item.type in series_types
        ]

    return results


def main():
    def with_additional_args(parser: argparse.ArgumentParser):
        parser.add_argument(
            "-o, --offset",
            help="Set episode name offset (eg. -o 3 will name the first episode 4). Useful for when a season is split into multiple parts.",
            type=int,
            nargs="?",
            required=False,
            default=0,
            dest="offset",
        )

    def get_download_sites(ctx: DownloadSitesCtx):
        anime_id = get_anime_id(ctx.series_name)
        if anime_id is None:
            Console.log_error(f"Can't fetch page for `{ctx.series_name}'")
            exit(1)

        episodes = get_anime_episode_list(anime_id)
        if ctx.episode_number not in episodes:
            raise EpisodeNumberNotFoundException()

        return get_anime_episode_download_server_list(
            episodes[ctx.episode_number],
            ctx.series_types,
        )

    downloader_main(
        site_name="zoro.to",
        with_additional_args=with_additional_args,
        allowed_series_types=ALLOWED_SERIES_TYPES,
        episode_page_url_fn=get_zoroto_page_url,
        download_sites_fn=get_download_sites,
        unwanted_cdn_hostnames=UNWANTED_CDN_HOSTNAMES,
    )


if __name__ == "__main__":
    main()
