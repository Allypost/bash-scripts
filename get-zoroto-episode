#!/usr/bin/env python3

import argparse
import atexit
import json
import multiprocessing
import re
import signal
import subprocess
from dataclasses import dataclass
from itertools import chain
import sys
from typing import Dict

import cloudscraper
from bs4 import BeautifulSoup
from bs4.element import ResultSet

from python.downloaders import get_download_info, sort_download_links
from python.helpers.main import get_episode_number_to_download, parse_arguments
from python.log.console import Chalk, Console

BASE_URL = "https://aniwatch.to"

ALLOWED_SERIES_TYPES = {
    "sub",
    "dub",
    "raw",
}


def get_zoroto_page_url(anime_name: str) -> str:
    return f"{BASE_URL}/watch/{anime_name}"


def get_anime_id(anime_name: str) -> str | None:
    Console.log_dim("Extracting id from name", return_line=True)
    name_parts = anime_name.split("-")
    anime_id = name_parts[-1]
    if anime_id.isdigit():
        return anime_id
    Console.log_dim("Fetching episode page...", return_line=True)
    response = cloudscraper.create_scraper().get(
        get_zoroto_page_url(anime_name),
        headers={"User-Agent": "Zoro.to stream video downloader"},
    )

    if not response.status_code:
        Console.log_dim("Anime not found")
        return None

    page_html = response.text

    Console.log_dim("Parsing HTML...", return_line=True)

    try:
        info = (
            BeautifulSoup(
                page_html,
                "html.parser",
            )
            .find(
                id="syncData",
            )
            .text
        )

        return json.loads(info)["anime_id"]
    except Exception:
        return None


@dataclass
class EpisodeInfo:
    number: float
    title: str
    id: str
    url: str


def get_page(url: str):
    return cloudscraper.create_scraper().get(
        url,
        headers={
            "Accept": "*/*",
            "X-Requested-With": "XMLHttpRequest",
            "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36",
        },
    )


def get_anime_episode_list(anime_id: str) -> Dict[float, str]:
    Console.log_dim("Fetching episode list...", return_line=True)
    response = get_page(
        f"{BASE_URL}/ajax/v2/episode/list/{anime_id}",
    )

    if not response.status_code:
        Console.log_dim("No episodes found")
        return {}

    page_json = response.json()

    elements = BeautifulSoup(
        page_json["html"],
        "html.parser",
    ).find_all(
        class_="ep-item",
    )

    if not elements:
        Console.log_dim("No info found")
        return {}

    return {
        float(element.attrs["data-number"]): EpisodeInfo(
            id=element.attrs["data-id"],
            url=element.attrs["href"],
            number=float(element.attrs["data-number"]),
            title=element.attrs["title"],
        )
        for element in elements
    }


@dataclass
class DownloadServerInfo:
    type: str
    id: str
    server_id: str
    name: str


@dataclass
class DownloadInfo:
    name: str
    type: str
    url: str


def get_download_url_from_server_info(
    server: DownloadServerInfo,
) -> DownloadInfo | None:
    response = get_page(
        f"{BASE_URL}/ajax/v2/episode/sources?id={server.id}",
    )

    if not response.status_code:
        return None

    page_json = response.json()

    return DownloadInfo(
        name=server.name,
        type=server.type,
        url=page_json["link"],
    )


def get_anime_episode_download_server_list(
    episode: EpisodeInfo,
    series_type: str = "sub",
) -> Dict[str, DownloadServerInfo] | None:
    Console.log_dim("Fetching download servers...", return_line=True)
    response = get_page(
        f"{BASE_URL}/ajax/v2/episode/servers?episodeId={episode.id}",
    )

    if not response.status_code:
        Console.log_dim("No download servers found")
        return None

    page_json = response.json()

    elements: ResultSet = BeautifulSoup(
        page_json["html"],
        "html.parser",
    ).find_all(
        class_="server-item",
    )

    servers_infos = {
        element.attrs["data-id"]: DownloadServerInfo(
            id=element.attrs["data-id"],
            name=next(element.stripped_strings),
            server_id=element.attrs["data-server-id"],
            type=element.attrs["data-type"],
        )
        for element in elements
    }

    Console.log_dim("Fetching list of source pages...", return_line=True)

    with multiprocessing.Pool() as pool:
        results = [
            item
            for item in pool.imap_unordered(
                get_download_url_from_server_info,
                servers_infos.values(),
            )
            if item and item.type == series_type
        ]

    return results


def main():
    Console.hide_cursor()
    atexit.register(Console.show_cursor)
    signal.signal(signal.SIGINT, lambda *_: sys.exit(0))
    signal.signal(signal.SIGTERM, lambda *_: sys.exit(0))
    signal.signal(signal.SIGHUP, lambda *_: sys.exit(0))

    def with_additional_args(parser: argparse.ArgumentParser):
        parser.add_argument(
            "-t --type",
            help="Whether to download the sub or dub",
            type=str,
            nargs="?",
            dest="series_type",
        )

        parser.add_argument(
            "-o, --offset",
            help="Set episode name offset (eg. -o 3 will name the first episode 4). Useful for when a season is split into multiple parts.",
            type=int,
            nargs="?",
            required=False,
            default=0,
            dest="offset",
        )

    argv = parse_arguments(site_name="zoro.to", extend=with_additional_args)

    series_name = argv.series_name or argv.series
    number_format = argv.number_format
    series_type = argv.series_type

    if series_name is None:
        raise Exception("No series name")

    if series_type not in ALLOWED_SERIES_TYPES:
        raise Exception(
            f"Invalid series type (must be one of: {', '.join(ALLOWED_SERIES_TYPES)})"
        )

    episode_number_offset = argv.offset
    episode_number = get_episode_number_to_download(argv) - episode_number_offset

    offset_str = (
        f" (offset episode {episode_number + episode_number_offset})"
        if episode_number_offset
        else ""
    )

    Console.log(
        f"Downloading {Chalk.badge(series_name, Chalk.black, Chalk.bg_yellow_bright)} episode {Chalk.badge(str(episode_number) + offset_str, Chalk.black, Chalk.bg_blue_bright)}"
    )

    anime_id = get_anime_id(series_name)
    if anime_id is None:
        Console.log_error(f"Can't fetch page for `{series_name}'")
        exit(1)

    episodes = get_anime_episode_list(anime_id)
    if episode_number not in episodes:
        Console.log_error(f"Episode {episode_number}{offset_str} can't be found")
        exit(1)

    download_sites = get_anime_episode_download_server_list(
        episodes[episode_number],
        series_type,
    )

    if not download_sites:
        Console.log_error(f"Couldn't download {episode_number}")
        exit(1)

    sorted_download_site_urls = sort_download_links(
        download_sites,
        to_url=lambda x: x.url,
    )

    download_sites = {item.name: item.url for item in sorted_download_site_urls}

    Console.log_dim("Trying to find download link...", return_line=True)

    processed = 0
    for site in download_sites:
        processed += 1
        download_url = download_sites[site]
        try:
            download_info = get_download_info(
                download_url, get_zoroto_page_url(anime_name=series_name)
            )
            if download_info is None:
                raise Exception("No handler")

            url = download_info.url
            referer = download_info.referer or url
            output_file = (
                f"{number_format % (episode_number + episode_number_offset)}.mp4"
            )

            Console.log(
                f"{Chalk.colour(Chalk.italic)}Downloading from {site}{Chalk.colour('23m')}"
            )

            Console.log_dim("Starting download...", return_line=True)
            download_cmd = [
                "yt-dlp",
                "--no-warnings",
                "--no-check-certificate",
                "--concurrent-fragments",
                "16",
                "--abort-on-unavailable-fragments",
                "--retries",
                "infinite",
                "--downloader",
                "ffmpeg",
                "--downloader-args",
                "-progress - -nostats",
                "--referer",
                referer,
                *list(
                    chain(
                        *[["--add-header", header] for header in download_info.headers]
                    )
                ),
                "--output",
                output_file,
                url,
            ]
            download_line_native = re.compile(r"\[download\]\s+(\d+\.\d+%.*)")

            with subprocess.Popen(
                download_cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.DEVNULL,
                bufsize=1,
                text=True,
            ) as proc:
                Console.log_dim("Waiting for download progress...", return_line=True)
                for line in proc.stdout:
                    # if download_line_native.search(line):
                    #     status = line.split(" ", maxsplit=1)[1].strip()
                    #     Console.log_dim(status, return_line=True)
                    #     continue

                    # Read lines until status stuff comes along
                    if line[0] == "[":
                        continue
                    break

                # Read status block
                def s(key, pad=0):
                    return output_status_info.get(key, "???").rjust(pad, " ")

                def format_output():
                    out = []
                    for item in output_layout:
                        if isinstance(item, dict):
                            for name, s_args in item.items():
                                if s_args[0] not in output_status_info:
                                    continue
                                out.append(f"{name}={s(*s_args)}")
                            continue
                        out.append(item)
                    return " ".join(out)

                output_status_info = {}
                output_layout = [
                    {"time": ("out_time",)},
                    "|",
                    {
                        "fps": ("fps", 6),
                        "speed": ("speed", 6),
                        "bitrate": ("bitrate", 13),
                    },
                    "|",
                    {"size": ("total_size",)},
                ]
                while proc.poll() is None:
                    try:
                        for line in proc.stdout:
                            if line.startswith("progress="):
                                break
                            key, value = line.strip().split("=", maxsplit=1)
                            output_status_info[key.strip()] = value.strip()
                        Console.log_dim(
                            format_output(),
                            return_line=True,
                        )
                    except Exception:
                        pass

                if proc.wait() != 0:
                    print(subprocess.list2cmdline(download_cmd))
                    raise Exception("Something broke")

            download_info.after_dl(output_file, download_info)
        except Exception as e:
            if str(e) == "No handler":
                continue

            print(("\n" + "=" * 32) * 2)
            print(e)
            print(("=" * 32 + "\n") * 2)

        for _ in range(processed):
            Console.clear_line()
            Console.move_up(1)
        Console.log_success(f"Episode {episode_number} downloaded")
        exit()

    Console.clear_line()
    Console.move_up(processed)
    Console.log_error(f"Failed to download episode {episode_number}")
    exit(1)


if __name__ == "__main__":
    main()
