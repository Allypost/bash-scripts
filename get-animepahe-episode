#!/usr/bin/env python3

import argparse
import random
import re
import string
import time
from dataclasses import dataclass
from typing import Union

import cloudscraper
from bs4 import BeautifulSoup

from python.helpers.main import (
    DownloadSite,
    DownloadSitesCtx,
    EpisodeNumberNotFoundException,
    downloader_main,
)
from python.helpers.retried_download import retried_download
from python.log.console import Console

BASE_URL = "https://animepahe.ru"

ALLOWED_SERIES_TYPES = {
    "jpn",
    "eng",
    "unknown",
}

UNWANTED_CDN_HOSTNAMES = {
    ".netmagcdn.com",
}


def get_animepahe_anime_url(anime_id: str) -> str:
    return f"{BASE_URL}/a/{anime_id}"


@dataclass
class EpisodeInfo:
    anime_id: str
    number: float
    title: str
    id: str
    session: str
    is_filler: bool = False


def the_cookie():
    rand_str = "".join(random.choices(string.ascii_letters + string.digits, k=16))
    return f"__ddg2_={rand_str}"


def create_scraper():
    return cloudscraper.create_scraper(
        browser={
            "desktop": True,
        },
    )


def get_anime_episode_list(anime_session_id: str) -> dict[float, EpisodeInfo]:
    Console.log_dim("Fetching episode list...", return_line=True)
    ret = {}
    page = 1
    page_try = 0
    while True:
        Console.log_dim(f"Fetching page {page} from API", return_line=True)
        scraper = create_scraper()
        response = scraper.get(
            f"{BASE_URL}/api?m=release&id={anime_session_id}&sort=episode_desc&page={page}",
            headers={
                "Cookie": the_cookie(),
            },
        )

        if response.status_code == 403:
            page_try += 1
            Console.log_dim(
                f"Got 403 for episode list API. Retrying... (Attempt {page_try})",
                return_line=True,
            )
            time.sleep(0.3 + 0.2 * page_try)
            continue
        page_try = 0

        if response.status_code == 404:
            Console.log_dim(
                f"Got 404 for episode list API. Check if ID is correct: {BASE_URL}/anime/{anime_session_id}"
            )
            return {}

        if not response.ok:
            Console.log_dim("Couldn't fetch API info")
            return {}

        Console.log_dim(f"Parsing info for page {page}", return_line=True)
        page += 1
        response = response.json()
        if not isinstance(response, dict):
            break
        if "data" not in response:
            break
        data = response["data"]
        if not data:
            break

        for item in data:
            ep = float(item["episode"])
            ret[ep] = EpisodeInfo(
                anime_id=anime_session_id,
                number=ep,
                title=item["title"],
                id=item["id"],
                session=item["session"],
                is_filler=bool(item["filler"]),
            )

        if "next_page_url" not in response or not response["next_page_url"]:
            break

    return ret


def get_anime_episode_download_server_list(
    episode_info: EpisodeInfo,
) -> list[DownloadSite] | None:
    response = retried_download(
        "download servers",
        lambda: create_scraper().get(
            f"{BASE_URL}/play/{episode_info.anime_id}/{episode_info.session}",
            headers={
                "Cookie": the_cookie(),
            },
        ),
    )

    if not response:
        return None

    page_html = response.text

    page_el = BeautifulSoup(
        page_html,
        "html.parser",
    )

    elements = page_el.select("#pickDownload .dropdown-item")

    def fix_url(url: Union[str, None]) -> Union[str, None]:
        if not url:
            return None

        if re.compile(r"^https?://").match(url):
            return url

        if str(url).startswith("//"):
            return f"https:{url}"

        Console.log(f"Unknown url format: `{url}'")

        return None

    resolution_regex = re.compile(r"(?:\W|^)(\d+)p(?:\W|$)")
    ret: list[DownloadSite] = []
    for element in elements:
        embed_url = fix_url(element.attrs.get("href"))
        if not embed_url:
            continue

        site_name = next(element.stripped_strings)
        audio = str(element.attrs.get("data-audio", "jpn"))
        resolution = 0
        resolution_match = resolution_regex.search(site_name)
        if resolution_match:
            resolution = int(resolution_match.group(1))

        ret.append(
            DownloadSite(
                name=site_name,
                url=embed_url,
                type=audio,
                other={"resolution": resolution},
            )
        )

    ret = list(sorted(ret, key=lambda x: -x.other.get("resolution", 0)))

    return ret


def get_anime_session_id(anime_name: str) -> str | None:
    # if it is not just a number, assume it's an anime ID
    if not re.compile(r"^\d+$").match(anime_name):
        return anime_name

    response = retried_download(
        name="anime page",
        do_request=lambda: create_scraper().get(
            f"{BASE_URL}/a/{anime_name}",
            allow_redirects=True,
            headers={
                "cookie": the_cookie(),
            },
        ),
    )

    page_html = response.text

    page_el = BeautifulSoup(
        page_html,
        "html.parser",
    )

    meta_url_el = page_el.select_one('meta[property="og:url"]')
    if not meta_url_el:
        Console.log_dim("Couldn't find anime ID in page")
        return None

    return meta_url_el.attrs.get("content", "").split("/")[-1]


def main():
    def with_additional_args(parser: argparse.ArgumentParser):
        parser.add_argument(
            "-o, --offset",
            help="Set episode name offset (eg. -o 3 will name the first episode 4). Useful for when a season is split into multiple parts.",
            type=int,
            nargs="?",
            required=False,
            default=0,
            dest="offset",
        )

    def get_download_sites(ctx: DownloadSitesCtx):
        anime_id = get_anime_session_id(ctx.series_name)
        if anime_id is None:
            Console.log_error(f"Can't fetch page for `{ctx.series_name}'")
            exit(1)

        episodes = get_anime_episode_list(anime_id)
        if ctx.episode_number not in episodes:
            raise EpisodeNumberNotFoundException()

        return get_anime_episode_download_server_list(
            episodes[ctx.episode_number],
        )

    downloader_main(
        site_name="animepahe.ru",
        with_additional_args=with_additional_args,
        allowed_series_types=ALLOWED_SERIES_TYPES,
        episode_page_url_fn=get_animepahe_anime_url,
        download_sites_fn=get_download_sites,
        unwanted_cdn_hostnames=UNWANTED_CDN_HOSTNAMES,
    )


if __name__ == "__main__":
    main()
